{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UBS Challenge Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Name: \"\" (empty string)\n",
    "\n",
    "Group Participants: ...\n",
    "\n",
    "The culmination of your submission should be a technical report that outlines your analytical journey, highlighting the methodologies employed, any obstacles encountered along the way, and the strategies adopted to\n",
    "overcome these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Challenge Description:\n",
    "\n",
    "We were tasked with ...\n",
    "\n",
    "## Report Overview\n",
    "\n",
    "1. Data Understanding\n",
    "   1. Data Cleaning and Preprocessing\n",
    "   2. Assumptions on Data\n",
    "   3. Feature Engineering and Data Augmentation\n",
    "2. Modelling Approach\n",
    "   1. Model Selection\n",
    "   2. Recommendation for Model Enhancement\n",
    "3. Actionable Insights\n",
    "   1. Company-Level\n",
    "   2. Industry-Level\n",
    "   3. Market-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/maximhuber/Developer/datathon/docs/../data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = os.path.join(os.path.join(os.getcwd(), os.pardir),\"data\")\n",
    "print(path)\n",
    "file = os.path.join(path, \"skylab_instagram_datathon_dataset.csv\")\n",
    "data = pd.read_csv(file, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a detailed account of the initial steps taken to prepare the data for analysis.\n",
    "\n",
    "This should include a description of how data quality issues, such as missing values or outliers, were addressed.\n",
    "\n",
    "1. Exploratory Data Analysis\n",
    "   - Examination and understanding of the dataset's structure and content\n",
    "   - Performing exploratory data analysis to understand:\n",
    "     - data patterns, \n",
    "     - outliers, and\n",
    "     - relationships between variables\n",
    "\n",
    "2. Data Cleaning\n",
    "   - Data preprocessing, include, but not limited to:\n",
    "     - handling missing values, \n",
    "     - data conversions, and \n",
    "     - normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preprocessing\n",
    "\n",
    "A general issue data scientists face is data cleaning. This is why we spent efforts on understanding our dataset and cleaning it.\n",
    "\n",
    "The biggest issues we faced:\n",
    "\n",
    "1. Unordered data (time-series, group by company...)\n",
    "2. NaNs\n",
    "3. Duplicate rows for compset\n",
    "4. Mappings between compset_groups, compsets, business_entities and so on.\n",
    "5. company acquisitions\n",
    "6. data normalization -> maybe explain a bit (only for k-means)\n",
    "\n",
    "#### Grouping and sorting data\n",
    "\n",
    "Before doing anything, we made sure we had order in our dataset.\n",
    "\n",
    "We made sure to group the data by `business_entity_doing_business_as_name`. Then we made sure to order the values for each business item by increasing `period_end_date` (first also transforming it to a pandas date format, I think)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure if code is necessary for this, it's really straightforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adressing NaNs\n",
    "\n",
    "We figured that ... % of data is NaN values. Due to the \"big\" amount of missing data, we decided that imputing the data makes more sense than dropping incomplete rows.\n",
    "\n",
    "Because we worked with time-series data, we thought that it is suitable to linearly interpolate missing numerical data between adjacent values. This aligns with the goal of finding outliers - linearly interpolating continues trends and should not cause , which might be observed when imputing with the mean over a bigger time-span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .... Copy in Michal's code for interpolating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nearly-Duplicate Rows\n",
    "\n",
    "Next, we realized that for some rows, we had multiple entries for ....\n",
    "\n",
    "We therefore mapped individual rows which only different in the `compset` column to have list in the `compset` column. This step is important because if we compute means over whole industries or the entire market, the computation would be biased towards companies which are present in multiple competitive sets. As a side effect, we were also able to reduce the size of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Code where maxim transformed `compset` into lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "In a following step, we really tried to understand how our data is structured and what it reveals about the real world. For this, we \n",
    "\n",
    "- first needed to generate mappings between compset_group, compset, business_entity name etc...\n",
    "\n",
    "After, we could\n",
    "- compare industries to the whole market\n",
    "- compare compsets within compset_groups\n",
    "- compare companies within compset\n",
    "\n",
    "#### Industries and Sub-Industries\n",
    "\n",
    "First, we made sure we understood what industries we are working with. We iterated over the csv file and found the following (shortened) hierarchical structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luxury & Premium & Mainstream\n",
      "['US Softlines Analyst Interest List', 'Luxury & Premium & Mainstream', 'Footwear', 'Premium Brands', 'Soft Luxury', 'Hard Luxury', 'Mid-Range Watch & Jewelry', 'Mainstream Brands', 'Global Luxury Analysts Interest List']\n",
      "\n",
      "Restaurants\n",
      "['Fast Casual', 'Restaurants', 'Casual Dining', 'QSR', 'Coffee']\n",
      "\n",
      "Beverages\n",
      "['Energy drinks', 'Alcohol', 'Beverages', 'Soda', 'Sports drinks']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Code showing some hierarchical structure for industries. Here, we could maybe make some nice tree plots or so that \"haut sie vom hocker\"\n",
    "\n",
    "import json\n",
    "\n",
    "# Map from compset_group -> compset\n",
    "map_path = '../out/compset_group_to_compset_map.json'\n",
    "with open(map_path, 'r') as f:\n",
    "    map_data = json.load(f)\n",
    "\n",
    "counter = 0  # Limit the printing\n",
    "for key, value in map_data.items():\n",
    "    print(key)\n",
    "    print(value)\n",
    "    print()  # Add an empty line after each key's values\n",
    "    counter += 1  # Increment the counter\n",
    "    if counter == 3:  # Check if we've printed 3 key-value pairs\n",
    "        break  # If yes, break out of the loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a feel for the market\n",
    "\n",
    "Next, we made sure we understood broad trends between industries. For visualization purposes, we restrict ourselves to \n",
    "five industries. They are ... . They exhibit these ... special features, which make them interesting for plotting.\n",
    "\n",
    "TODO: add plots that show e.g. follower count means over industries, as box-plots and time-series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Assumptions\n",
    "\n",
    "Clearly articulate any assumptions that were made during the data preparation phase.\n",
    "\n",
    "Here, we need to think a little more. Short and concise, but it's also important to have good stuff here.\n",
    "\n",
    "Assumptions made so far:\n",
    "- Post data is not only advertised stuff but also general posts on the company profiles\n",
    "Impact: more accurate representation of customer sentiment towards the brand because people tend to like and comment on content that is marked as sponsored.\n",
    "- COVID ??\n",
    "- no company acquisitions (which holds for most of the data, but not for all (e.g. LVMH)) -> make some plots confirming this (I think Dior was acquired by LVMH, and maybe we see something in the time-series plot after the buy date...). the least we could do would be to identify all holding changes. i think we can do this based on the maps I created or create some more \n",
    "- for the k-means model, we work (!!) with 2023 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Feature Engineering and Data Augmentation\n",
    "\n",
    "Describe any techniques employed to enhance the dataset, whether through the creation of new features or augmentation of the existing data.\n",
    "\n",
    "Goal: create relevant features for identifying deviations\n",
    "\n",
    "TODO: fett reinschreiben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Approach\n",
    "\n",
    "### i. Model Selection\n",
    "\n",
    "Explain on the choice(s) of statistical or machine learning model(s) utilized in your analysis. Provide a compelling justification for each model selected, emphasizing how they align with the objectives of the challenge.\n",
    "\n",
    "How could we go about solving this challenge?\n",
    "\n",
    "In the end, we came up with three models.\n",
    "\n",
    "1. Baseline: Best ranked companies according to engagement rate\n",
    "2. k-means: ?\n",
    "3. ?\n",
    "\n",
    "In the following, we detail why our model choices make sense in the broader setting.\n",
    "\n",
    "#### 1. Baseline Model\n",
    "\n",
    "How it works:\n",
    "- for each month, we ranked companies according to their mean engagement rates\n",
    "- in the end, we averaged the company rankings and took the top and bottom three as outliers\n",
    "\n",
    "Reasons:\n",
    "- need something to start with\n",
    "- measure consistency and social media engagement (relative to followers), which is one of the most relevant metrics used in social media marketing.\n",
    "- gives insight into best-performing and worst-performing companies, which can serve as indicators for investment opportunities and risk management, which we will explore more **in a later section**\n",
    "\n",
    "#### 2. K-Means Model\n",
    "\n",
    "How it works:\n",
    "...\n",
    "\n",
    "\n",
    "Reasons (standalone):\n",
    "- ...\n",
    "\n",
    "Why we chose this model over other models:\n",
    "- ...\n",
    "- ...\n",
    "- ...\n",
    "\n",
    "#### 3. ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Recommendation for Model Enhancement\n",
    "\n",
    "Conclude with a thoughtful reflection on potential avenues for further improving your model. Propose specific modifications or additional analyses that could refine your predictions and insights.\n",
    "\n",
    "More features would be possible if we had:\n",
    "\n",
    "- customer lifetime value (CLV) in social media terms (ask gpt)\n",
    "- customer acquisition cost (CAC) in social media terms (ask gpt)\n",
    "- ratio CLV / CAC\n",
    "- some more metrics, like reach ....\n",
    "\n",
    "From these features we would have the chance to have a broader feature set, which would enhance model accuracy and understanding of company performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Actionable Insights\n",
    "\n",
    "In the process of finding outliers within the given dataset, we came to an understanding of the trends that govern social media performance of the given firms.\n",
    "\n",
    "We need insights into:\n",
    "\n",
    "1. identify outlier firms that are performing exceptionally well on social media\n",
    "  \n",
    "    -> this could indicate strong brand engagement and customer loyalty\n",
    "    -> might translate into future profitability\n",
    "    -> investment opportunities\n",
    "\n",
    "    find why they are well-performing?\n",
    "    \n",
    "    possible causes:\n",
    "    - compare to industry?\n",
    "    - long & strong brand history\n",
    "    - strong brand ambassadors\n",
    "    - good ethics of company (planting trees etc)\n",
    "    - funny social media accounts whatever\n",
    "\n",
    "2. identify outlier firms on lower end\n",
    "\n",
    "    -> signals issues with brand perception, customer engagement, or emerging crises\n",
    "    -> poses risk to company's long-term stability \n",
    "    -> requires proactive (risk) management.\n",
    "\n",
    "    why are they not well-performing?\n",
    "    - industry?\n",
    "    - scandals?\n",
    "    - stock development?\n",
    "\n",
    "3. identify market trends, industry trends\n",
    "\n",
    "    - this is actually more of the data exploration part, but we need to synthesize the findings into the insights report because other groups might not do it, so it gives us an edge\n",
    "  \n",
    "    -> can indicate broader market trends / shifts in consumer behaviour\n",
    "    -> can help more informed broadcasting & market analysis\n",
    "    -> more of a tool to use as input to more complex market models\n",
    "    -> can probably work well together with recent LLMs that consume market news/sentiment because social media is a good representation of what people think\n",
    "\n",
    "4. identify actual weird outlier firms -> can generate solid investment advice, probably\n",
    "\n",
    "    -> idea with k-means clustering: have them clustered according to performance features\n",
    "    -> then find \"bad stock performance\" companies in \"good social media performance\" companies and vice versa\n",
    "\n",
    "5. general UBS use of our analysis: advise their customers why their competitors are outliers, which can help the clients refine their competitive strategies. Identifying key factors leading to higher engagement can inform better marketing strategies. & also: Product and Service Development: Insights derived from social media performance can influence decisions about product innovations or adjustments, focusing on areas that resonate with or displease the market."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datathon_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
